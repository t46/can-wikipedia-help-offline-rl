{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dc26468-a7a6-4cc5-b549-1d494c6b1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import argparse\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append('/Users/shiro/research/projects/rl-nlp/can-wikipedia-help-offline-rl/code')\n",
    "\n",
    "from decision_transformer.evaluation.evaluate_episodes import (\n",
    "    evaluate_episode,\n",
    "    evaluate_episode_rtg,\n",
    ")\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "from decision_transformer.models.mlp_bc import MLPBCModel\n",
    "from decision_transformer.training.act_trainer import ActTrainer\n",
    "from decision_transformer.training.seq_trainer import SequenceTrainer\n",
    "\n",
    "from utils import get_optimizer\n",
    "import os\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0ba1ea9-6c8a-4201-a58a-5c2aea67e1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "==================================================\n",
      "Loading from pretrained\n",
      "Loaded from ../checkpoints/gpt2_medium_hopper_666/model_40.pt\n"
     ]
    }
   ],
   "source": [
    "seed = 666\n",
    "\n",
    "model_name = False\n",
    "\n",
    "env_name = 'hopper'\n",
    "\n",
    "layer = 11\n",
    "batchid = 0\n",
    "\n",
    "# env_name = 'walker2d'\n",
    "dataset_name = 'medium'\n",
    "\n",
    "pretrained_lm = 'gpt2'\n",
    "\n",
    "variant = {\n",
    "    'embed_dim': 128,\n",
    "    'n_layer': 3,\n",
    "    'n_head': 1,\n",
    "    'activation_function': 'relu',\n",
    "    'dropout': 0.2, # 0.1\n",
    "    'load_checkpoint': f'../checkpoints/gpt2_medium_hopper_666/model_40.pt',\n",
    "    'seed': seed,\n",
    "    'outdir': f\"checkpoints/{model_name}_{dataset_name}_{env_name}_{seed}\",\n",
    "    'env': env_name,\n",
    "    'dataset': dataset_name,\n",
    "    'model_type': 'dt',\n",
    "    'K': 20, # 2\n",
    "    'pct_traj': 1.0,\n",
    "    'batch_size': 100,  # 64\n",
    "    'num_eval_episodes': 100,\n",
    "    'max_iters': 40,\n",
    "    'num_steps_per_iter': 2500,\n",
    "    'pretrained_lm': pretrained_lm,\n",
    "    'gpt_kmeans': None,  # 1000 <-- メモリに乗らない\n",
    "    'kmeans_cache': None,  # 'kmeans_cache/gpt2_lm_1000.pt'\n",
    "    'frozen': False,\n",
    "    'extend_positions': False,\n",
    "    'share_input_output_proj': True\n",
    "}\n",
    "\n",
    "torch.manual_seed(variant[\"seed\"])\n",
    "os.makedirs(variant[\"outdir\"], exist_ok=True)\n",
    "# device = variant.get(\"device\", \"cuda\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_to_wandb = variant.get(\"log_to_wandb\", False)\n",
    "\n",
    "env_name, dataset = variant[\"env\"], variant[\"dataset\"]\n",
    "model_type = variant[\"model_type\"]\n",
    "exp_prefix = 'gym-experiment'\n",
    "group_name = f\"{exp_prefix}-{env_name}-{dataset}\"\n",
    "exp_prefix = f\"{group_name}-{random.randint(int(1e5), int(1e6) - 1)}\"\n",
    "\n",
    "if env_name == \"hopper\":\n",
    "    env = gym.make(\"Hopper-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "    scale = 1000.0  # normalization for rewards/returns\n",
    "elif env_name == \"halfcheetah\":\n",
    "    env = gym.make(\"HalfCheetah-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [12000, 6000]\n",
    "    scale = 1000.0\n",
    "elif env_name == \"walker2d\":\n",
    "    env = gym.make(\"Walker2d-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [5000, 2500]\n",
    "    scale = 1000.0\n",
    "elif env_name == \"reacher2d\":\n",
    "    from decision_transformer.envs.reacher_2d import Reacher2dEnv\n",
    "\n",
    "    env = Reacher2dEnv()\n",
    "    max_ep_len = 100\n",
    "    env_targets = [76, 40]\n",
    "    scale = 10.0\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if model_type == \"bc\":\n",
    "    env_targets = env_targets[\n",
    "        :1\n",
    "    ]  # since BC ignores target, no need for different evaluations\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# load dataset\n",
    "dataset_path = f\"../data/{env_name}-{dataset}-v2.pkl\"\n",
    "with open(dataset_path, \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "# save all path information into separate lists\n",
    "mode = variant.get(\"mode\", \"normal\")\n",
    "states, traj_lens, returns = [], [], []\n",
    "for path in trajectories:\n",
    "    if mode == \"delayed\":  # delayed: all rewards moved to end of trajectory\n",
    "        path[\"rewards\"][-1] = path[\"rewards\"].sum()\n",
    "        path[\"rewards\"][:-1] = 0.0\n",
    "    states.append(path[\"observations\"])\n",
    "    traj_lens.append(len(path[\"observations\"]))\n",
    "    returns.append(path[\"rewards\"].sum())\n",
    "traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "# used for input normalization\n",
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "num_timesteps = sum(traj_lens)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Starting new experiment: {env_name} {dataset}\")\n",
    "print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "K = variant[\"K\"]\n",
    "batch_size = variant[\"batch_size\"]\n",
    "num_eval_episodes = variant[\"num_eval_episodes\"]\n",
    "pct_traj = variant.get(\"pct_traj\", 1.0)\n",
    "\n",
    "# only train on top pct_traj trajectories (for %BC experiment)\n",
    "num_timesteps = max(int(pct_traj * num_timesteps), 1)\n",
    "sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "num_trajectories = 1\n",
    "timesteps = traj_lens[sorted_inds[-1]]\n",
    "ind = len(trajectories) - 2\n",
    "while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "    timesteps += traj_lens[sorted_inds[ind]]\n",
    "    num_trajectories += 1\n",
    "    ind -= 1\n",
    "sorted_inds = sorted_inds[-num_trajectories:]\n",
    "\n",
    "# used to reweight sampling so we sample according to timesteps instead of trajectories\n",
    "p_sample = traj_lens[sorted_inds] / sum(traj_lens[sorted_inds])\n",
    "\n",
    "def discount_cumsum(x, gamma):\n",
    "    discount_cumsum = np.zeros_like(x)\n",
    "    discount_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "    return discount_cumsum\n",
    "\n",
    "def get_batch(batch_size=256, max_len=K):\n",
    "    batch_inds = np.random.choice(\n",
    "        np.arange(num_trajectories),\n",
    "        size=batch_size,\n",
    "        replace=True,\n",
    "        p=p_sample,  # reweights so we sample according to timesteps\n",
    "    )\n",
    "\n",
    "    s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "    for i in range(batch_size):\n",
    "        traj = trajectories[int(sorted_inds[batch_inds[i]])]\n",
    "        si = random.randint(0, traj[\"rewards\"].shape[0] - 1)\n",
    "\n",
    "        # get sequences from dataset\n",
    "        s.append(traj[\"observations\"][si : si + max_len].reshape(1, -1, state_dim))\n",
    "        a.append(traj[\"actions\"][si : si + max_len].reshape(1, -1, act_dim))\n",
    "        r.append(traj[\"rewards\"][si : si + max_len].reshape(1, -1, 1))\n",
    "        if \"terminals\" in traj:\n",
    "            d.append(traj[\"terminals\"][si : si + max_len].reshape(1, -1))\n",
    "        else:\n",
    "            d.append(traj[\"dones\"][si : si + max_len].reshape(1, -1))\n",
    "        timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "        timesteps[-1][timesteps[-1] >= max_ep_len] = (\n",
    "            max_ep_len - 1\n",
    "        )  # padding cutoff\n",
    "        rtg.append(\n",
    "            discount_cumsum(traj[\"rewards\"][si:], gamma=1.0)[\n",
    "                : s[-1].shape[1] + 1\n",
    "            ].reshape(1, -1, 1)\n",
    "        )\n",
    "        if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "            rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "        # padding and state + reward normalization\n",
    "        tlen = s[-1].shape[1]\n",
    "        s[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1\n",
    "        )\n",
    "        s[-1] = (s[-1] - state_mean) / state_std\n",
    "        a[-1] = np.concatenate(\n",
    "            [np.ones((1, max_len - tlen, act_dim)) * -10.0, a[-1]], axis=1\n",
    "        )\n",
    "        r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "        d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "        rtg[-1] = (\n",
    "            np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            / scale\n",
    "        )\n",
    "        timesteps[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1\n",
    "        )\n",
    "        mask.append(\n",
    "            np.concatenate(\n",
    "                [np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    s = torch.from_numpy(np.concatenate(s, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    a = torch.from_numpy(np.concatenate(a, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    r = torch.from_numpy(np.concatenate(r, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    d = torch.from_numpy(np.concatenate(d, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "    return s, a, r, d, rtg, timesteps, mask\n",
    "\n",
    "(\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    dones,\n",
    "    rtg,\n",
    "    timesteps,\n",
    "    attention_mask,\n",
    ") = get_batch(batch_size)\n",
    "action_target = torch.clone(actions)\n",
    "\n",
    "model_trained = DecisionTransformer(\n",
    "    args=variant,\n",
    "    state_dim=state_dim,\n",
    "    act_dim=act_dim,\n",
    "    max_length=K,\n",
    "    max_ep_len=max_ep_len,\n",
    "    hidden_size=variant[\"embed_dim\"],\n",
    "    n_layer=variant[\"n_layer\"],\n",
    "    n_head=variant[\"n_head\"],\n",
    "    n_inner=4 * variant[\"embed_dim\"],\n",
    "    activation_function=variant[\"activation_function\"],\n",
    "    n_positions=1024,\n",
    "    resid_pdrop=variant[\"dropout\"],\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "if variant[\"load_checkpoint\"]:\n",
    "    state_dict = torch.load(variant[\"load_checkpoint\"], map_location=torch.device('cpu'))\n",
    "    model_trained.load_state_dict(state_dict)\n",
    "    print(f\"Loaded from {variant['load_checkpoint']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26cd8aa3-9471-4f14-a998-522fcda5cd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: hopper medium\n",
      "2186 trajectories, 999906 timesteps found\n",
      "Average return: 1422.06, std: 378.95\n",
      "Max return: 3222.36, min: 315.87\n",
      "==================================================\n",
      "Loading from pretrained\n"
     ]
    }
   ],
   "source": [
    "seed = 666\n",
    "\n",
    "model_name = False\n",
    "\n",
    "env_name = 'hopper'\n",
    "\n",
    "layer = 11\n",
    "batchid = 0\n",
    "\n",
    "# env_name = 'walker2d'\n",
    "dataset_name = 'medium'\n",
    "\n",
    "pretrained_lm = 'gpt2'\n",
    "\n",
    "variant = {\n",
    "    'embed_dim': 128,\n",
    "    'n_layer': 3,\n",
    "    'n_head': 1,\n",
    "    'activation_function': 'relu',\n",
    "    'dropout': 0.2, # 0.1\n",
    "    'load_checkpoint': False,\n",
    "    'seed': seed,\n",
    "    'outdir': f\"checkpoints/{model_name}_{dataset_name}_{env_name}_{seed}\",\n",
    "    'env': env_name,\n",
    "    'dataset': dataset_name,\n",
    "    'model_type': 'dt',\n",
    "    'K': 20, # 2\n",
    "    'pct_traj': 1.0,\n",
    "    'batch_size': 100,  # 64\n",
    "    'num_eval_episodes': 100,\n",
    "    'max_iters': 40,\n",
    "    'num_steps_per_iter': 2500,\n",
    "    'pretrained_lm': pretrained_lm,\n",
    "    'gpt_kmeans': None,  # 1000 <-- メモリに乗らない\n",
    "    'kmeans_cache': None,  # 'kmeans_cache/gpt2_lm_1000.pt'\n",
    "    'frozen': False,\n",
    "    'extend_positions': False,\n",
    "    'share_input_output_proj': True\n",
    "}\n",
    "\n",
    "torch.manual_seed(variant[\"seed\"])\n",
    "os.makedirs(variant[\"outdir\"], exist_ok=True)\n",
    "# device = variant.get(\"device\", \"cuda\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log_to_wandb = variant.get(\"log_to_wandb\", False)\n",
    "\n",
    "env_name, dataset = variant[\"env\"], variant[\"dataset\"]\n",
    "model_type = variant[\"model_type\"]\n",
    "exp_prefix = 'gym-experiment'\n",
    "group_name = f\"{exp_prefix}-{env_name}-{dataset}\"\n",
    "exp_prefix = f\"{group_name}-{random.randint(int(1e5), int(1e6) - 1)}\"\n",
    "\n",
    "if env_name == \"hopper\":\n",
    "    env = gym.make(\"Hopper-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "    scale = 1000.0  # normalization for rewards/returns\n",
    "elif env_name == \"halfcheetah\":\n",
    "    env = gym.make(\"HalfCheetah-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [12000, 6000]\n",
    "    scale = 1000.0\n",
    "elif env_name == \"walker2d\":\n",
    "    env = gym.make(\"Walker2d-v3\")\n",
    "    max_ep_len = 1000\n",
    "    env_targets = [5000, 2500]\n",
    "    scale = 1000.0\n",
    "elif env_name == \"reacher2d\":\n",
    "    from decision_transformer.envs.reacher_2d import Reacher2dEnv\n",
    "\n",
    "    env = Reacher2dEnv()\n",
    "    max_ep_len = 100\n",
    "    env_targets = [76, 40]\n",
    "    scale = 10.0\n",
    "else:\n",
    "    raise NotImplementedError\n",
    "\n",
    "if model_type == \"bc\":\n",
    "    env_targets = env_targets[\n",
    "        :1\n",
    "    ]  # since BC ignores target, no need for different evaluations\n",
    "\n",
    "state_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.shape[0]\n",
    "\n",
    "# load dataset\n",
    "dataset_path = f\"../data/{env_name}-{dataset}-v2.pkl\"\n",
    "with open(dataset_path, \"rb\") as f:\n",
    "    trajectories = pickle.load(f)\n",
    "\n",
    "# save all path information into separate lists\n",
    "mode = variant.get(\"mode\", \"normal\")\n",
    "states, traj_lens, returns = [], [], []\n",
    "for path in trajectories:\n",
    "    if mode == \"delayed\":  # delayed: all rewards moved to end of trajectory\n",
    "        path[\"rewards\"][-1] = path[\"rewards\"].sum()\n",
    "        path[\"rewards\"][:-1] = 0.0\n",
    "    states.append(path[\"observations\"])\n",
    "    traj_lens.append(len(path[\"observations\"]))\n",
    "    returns.append(path[\"rewards\"].sum())\n",
    "traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "# used for input normalization\n",
    "states = np.concatenate(states, axis=0)\n",
    "state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "num_timesteps = sum(traj_lens)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"Starting new experiment: {env_name} {dataset}\")\n",
    "print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "K = variant[\"K\"]\n",
    "batch_size = variant[\"batch_size\"]\n",
    "num_eval_episodes = variant[\"num_eval_episodes\"]\n",
    "pct_traj = variant.get(\"pct_traj\", 1.0)\n",
    "\n",
    "# only train on top pct_traj trajectories (for %BC experiment)\n",
    "num_timesteps = max(int(pct_traj * num_timesteps), 1)\n",
    "sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "num_trajectories = 1\n",
    "timesteps = traj_lens[sorted_inds[-1]]\n",
    "ind = len(trajectories) - 2\n",
    "while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "    timesteps += traj_lens[sorted_inds[ind]]\n",
    "    num_trajectories += 1\n",
    "    ind -= 1\n",
    "sorted_inds = sorted_inds[-num_trajectories:]\n",
    "\n",
    "# used to reweight sampling so we sample according to timesteps instead of trajectories\n",
    "p_sample = traj_lens[sorted_inds] / sum(traj_lens[sorted_inds])\n",
    "\n",
    "def discount_cumsum(x, gamma):\n",
    "    discount_cumsum = np.zeros_like(x)\n",
    "    discount_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "    return discount_cumsum\n",
    "\n",
    "def get_batch(batch_size=256, max_len=K):\n",
    "    batch_inds = np.random.choice(\n",
    "        np.arange(num_trajectories),\n",
    "        size=batch_size,\n",
    "        replace=True,\n",
    "        p=p_sample,  # reweights so we sample according to timesteps\n",
    "    )\n",
    "\n",
    "    s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "    for i in range(batch_size):\n",
    "        traj = trajectories[int(sorted_inds[batch_inds[i]])]\n",
    "        si = random.randint(0, traj[\"rewards\"].shape[0] - 1)\n",
    "\n",
    "        # get sequences from dataset\n",
    "        s.append(traj[\"observations\"][si : si + max_len].reshape(1, -1, state_dim))\n",
    "        a.append(traj[\"actions\"][si : si + max_len].reshape(1, -1, act_dim))\n",
    "        r.append(traj[\"rewards\"][si : si + max_len].reshape(1, -1, 1))\n",
    "        if \"terminals\" in traj:\n",
    "            d.append(traj[\"terminals\"][si : si + max_len].reshape(1, -1))\n",
    "        else:\n",
    "            d.append(traj[\"dones\"][si : si + max_len].reshape(1, -1))\n",
    "        timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "        timesteps[-1][timesteps[-1] >= max_ep_len] = (\n",
    "            max_ep_len - 1\n",
    "        )  # padding cutoff\n",
    "        rtg.append(\n",
    "            discount_cumsum(traj[\"rewards\"][si:], gamma=1.0)[\n",
    "                : s[-1].shape[1] + 1\n",
    "            ].reshape(1, -1, 1)\n",
    "        )\n",
    "        if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "            rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "        # padding and state + reward normalization\n",
    "        tlen = s[-1].shape[1]\n",
    "        s[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1\n",
    "        )\n",
    "        s[-1] = (s[-1] - state_mean) / state_std\n",
    "        a[-1] = np.concatenate(\n",
    "            [np.ones((1, max_len - tlen, act_dim)) * -10.0, a[-1]], axis=1\n",
    "        )\n",
    "        r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "        d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "        rtg[-1] = (\n",
    "            np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            / scale\n",
    "        )\n",
    "        timesteps[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1\n",
    "        )\n",
    "        mask.append(\n",
    "            np.concatenate(\n",
    "                [np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    s = torch.from_numpy(np.concatenate(s, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    a = torch.from_numpy(np.concatenate(a, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    r = torch.from_numpy(np.concatenate(r, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    d = torch.from_numpy(np.concatenate(d, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "    return s, a, r, d, rtg, timesteps, mask\n",
    "\n",
    "(\n",
    "    states,\n",
    "    actions,\n",
    "    rewards,\n",
    "    dones,\n",
    "    rtg,\n",
    "    timesteps,\n",
    "    attention_mask,\n",
    ") = get_batch(batch_size)\n",
    "action_target = torch.clone(actions)\n",
    "\n",
    "model_initial = DecisionTransformer(\n",
    "    args=variant,\n",
    "    state_dim=state_dim,\n",
    "    act_dim=act_dim,\n",
    "    max_length=K,\n",
    "    max_ep_len=max_ep_len,\n",
    "    hidden_size=variant[\"embed_dim\"],\n",
    "    n_layer=variant[\"n_layer\"],\n",
    "    n_head=variant[\"n_head\"],\n",
    "    n_inner=4 * variant[\"embed_dim\"],\n",
    "    activation_function=variant[\"activation_function\"],\n",
    "    n_positions=1024,\n",
    "    resid_pdrop=variant[\"dropout\"],\n",
    "    attn_pdrop=0.1,\n",
    ")\n",
    "if variant[\"load_checkpoint\"]:\n",
    "    state_dict = torch.load(variant[\"load_checkpoint\"], map_location=torch.device('cpu'))\n",
    "    model_initial.load_state_dict(state_dict)\n",
    "    print(f\"Loaded from {variant['load_checkpoint']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a9cb001-4445-4a69-a4d3-f5395590eddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTransformer(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.2, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_timestep): Embedding(1000, 768)\n",
       "  (embed_return): Linear(in_features=1, out_features=768, bias=True)\n",
       "  (embed_state): Linear(in_features=11, out_features=768, bias=True)\n",
       "  (embed_action): Linear(in_features=3, out_features=768, bias=True)\n",
       "  (embed_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_initial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
