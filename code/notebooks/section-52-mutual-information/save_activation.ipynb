{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import sys\n",
    "\n",
    "sys.path.append('/root/projects/can-wikipedia-help-offline-rl/code')\n",
    "\n",
    "from decision_transformer.models.decision_transformer import DecisionTransformer\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_cumsum(x, gamma):\n",
    "    discount_cumsum = np.zeros_like(x)\n",
    "    discount_cumsum[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        discount_cumsum[t] = x[t] + gamma * discount_cumsum[t + 1]\n",
    "    return discount_cumsum\n",
    "\n",
    "def prepare_data(variant):\n",
    "    env_name, dataset = variant[\"env\"], variant[\"dataset\"]\n",
    "    model_type = variant[\"model_type\"]\n",
    "    exp_prefix = 'gym-experiment'\n",
    "    group_name = f\"{exp_prefix}-{env_name}-{dataset}\"\n",
    "    exp_prefix = f\"{group_name}-{random.randint(int(1e5), int(1e6) - 1)}\"\n",
    "\n",
    "    if env_name == \"hopper\":\n",
    "        env = gym.make(\"Hopper-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "        scale = 1000.0  # normalization for rewards/returns\n",
    "    elif env_name == \"halfcheetah\":\n",
    "        env = gym.make(\"HalfCheetah-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [12000, 6000]\n",
    "        scale = 1000.0\n",
    "    elif env_name == \"walker2d\":\n",
    "        env = gym.make(\"Walker2d-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [5000, 2500]\n",
    "        scale = 1000.0\n",
    "    elif env_name == \"reacher2d\":\n",
    "        from decision_transformer.envs.reacher_2d import Reacher2dEnv\n",
    "\n",
    "        env = Reacher2dEnv()\n",
    "        max_ep_len = 100\n",
    "        env_targets = [76, 40]\n",
    "        scale = 10.0\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if model_type == \"bc\":\n",
    "        env_targets = env_targets[\n",
    "            :1\n",
    "        ]  # since BC ignores target, no need for different evaluations\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # load dataset\n",
    "    dataset_path = f\"../data/{env_name}-{dataset}-v2.pkl\"\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        trajectories = pickle.load(f)\n",
    "\n",
    "    # save all path information into separate lists\n",
    "    mode = variant.get(\"mode\", \"normal\")\n",
    "    states, traj_lens, returns = [], [], []\n",
    "    for path in trajectories:\n",
    "        if mode == \"delayed\":  # delayed: all rewards moved to end of trajectory\n",
    "            path[\"rewards\"][-1] = path[\"rewards\"].sum()\n",
    "            path[\"rewards\"][:-1] = 0.0\n",
    "        states.append(path[\"observations\"])\n",
    "        traj_lens.append(len(path[\"observations\"]))\n",
    "        returns.append(path[\"rewards\"].sum())\n",
    "    traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "    # used for input normalization\n",
    "    states = np.concatenate(states, axis=0)\n",
    "    state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "    num_timesteps = sum(traj_lens)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Starting new experiment: {env_name} {dataset}\")\n",
    "    print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "    print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "    print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    pct_traj = variant.get(\"pct_traj\", 1.0)\n",
    "\n",
    "    # only train on top pct_traj trajectories (for %BC experiment)\n",
    "    num_timesteps = max(int(pct_traj * num_timesteps), 1)\n",
    "    sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "    num_trajectories = 1\n",
    "    timesteps = traj_lens[sorted_inds[-1]]\n",
    "    ind = len(trajectories) - 2\n",
    "    while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "        timesteps += traj_lens[sorted_inds[ind]]\n",
    "        num_trajectories += 1\n",
    "        ind -= 1\n",
    "    sorted_inds = sorted_inds[-num_trajectories:]\n",
    "\n",
    "    # used to reweight sampling so we sample according to timesteps instead of trajectories\n",
    "    p_sample = traj_lens[sorted_inds] / sum(traj_lens[sorted_inds])\n",
    "    \n",
    "    return trajectories, sorted_inds, state_dim, act_dim, max_ep_len, state_mean, state_std, num_trajectories, p_sample, scale\n",
    "\n",
    "def get_batch(\n",
    "    batch_size, \n",
    "    max_len,\n",
    "    trajectories,\n",
    "    sorted_inds,\n",
    "    state_dim,\n",
    "    act_dim,\n",
    "    max_ep_len,\n",
    "    state_mean,\n",
    "    state_std,\n",
    "    num_trajectories,\n",
    "    p_sample,\n",
    "    scale,\n",
    "    device\n",
    "    ):\n",
    "    batch_inds = np.random.choice(\n",
    "        np.arange(num_trajectories),\n",
    "        size=batch_size,\n",
    "        replace=True,\n",
    "        p=p_sample,  # reweights so we sample according to timesteps\n",
    "    )\n",
    "\n",
    "    s, a, r, d, rtg, timesteps, mask = [], [], [], [], [], [], []\n",
    "    for i in range(batch_size):\n",
    "        traj = trajectories[int(sorted_inds[batch_inds[i]])]\n",
    "        si = random.randint(0, traj[\"rewards\"].shape[0] - 1)\n",
    "\n",
    "        # get sequences from dataset\n",
    "        s.append(traj[\"observations\"][si : si + max_len].reshape(1, -1, state_dim))\n",
    "        a.append(traj[\"actions\"][si : si + max_len].reshape(1, -1, act_dim))\n",
    "        r.append(traj[\"rewards\"][si : si + max_len].reshape(1, -1, 1))\n",
    "        if \"terminals\" in traj:\n",
    "            d.append(traj[\"terminals\"][si : si + max_len].reshape(1, -1))\n",
    "        else:\n",
    "            d.append(traj[\"dones\"][si : si + max_len].reshape(1, -1))\n",
    "        timesteps.append(np.arange(si, si + s[-1].shape[1]).reshape(1, -1))\n",
    "        timesteps[-1][timesteps[-1] >= max_ep_len] = (\n",
    "            max_ep_len - 1\n",
    "        )  # padding cutoff\n",
    "        rtg.append(\n",
    "            discount_cumsum(traj[\"rewards\"][si:], gamma=1.0)[\n",
    "                : s[-1].shape[1] + 1\n",
    "            ].reshape(1, -1, 1)\n",
    "        )\n",
    "        if rtg[-1].shape[1] <= s[-1].shape[1]:\n",
    "            rtg[-1] = np.concatenate([rtg[-1], np.zeros((1, 1, 1))], axis=1)\n",
    "\n",
    "        # padding and state + reward normalization\n",
    "        tlen = s[-1].shape[1]\n",
    "        s[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen, state_dim)), s[-1]], axis=1\n",
    "        )\n",
    "        s[-1] = (s[-1] - state_mean) / state_std\n",
    "        a[-1] = np.concatenate(\n",
    "            [np.ones((1, max_len - tlen, act_dim)) * -10.0, a[-1]], axis=1\n",
    "        )\n",
    "        r[-1] = np.concatenate([np.zeros((1, max_len - tlen, 1)), r[-1]], axis=1)\n",
    "        d[-1] = np.concatenate([np.ones((1, max_len - tlen)) * 2, d[-1]], axis=1)\n",
    "        rtg[-1] = (\n",
    "            np.concatenate([np.zeros((1, max_len - tlen, 1)), rtg[-1]], axis=1)\n",
    "            / scale\n",
    "        )\n",
    "        timesteps[-1] = np.concatenate(\n",
    "            [np.zeros((1, max_len - tlen)), timesteps[-1]], axis=1\n",
    "        )\n",
    "        mask.append(\n",
    "            np.concatenate(\n",
    "                [np.zeros((1, max_len - tlen)), np.ones((1, tlen))], axis=1\n",
    "            )\n",
    "        )\n",
    "\n",
    "    s = torch.from_numpy(np.concatenate(s, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    a = torch.from_numpy(np.concatenate(a, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    r = torch.from_numpy(np.concatenate(r, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    d = torch.from_numpy(np.concatenate(d, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    rtg = torch.from_numpy(np.concatenate(rtg, axis=0)).to(\n",
    "        dtype=torch.float32, device=device\n",
    "    )\n",
    "    timesteps = torch.from_numpy(np.concatenate(timesteps, axis=0)).to(\n",
    "        dtype=torch.long, device=device\n",
    "    )\n",
    "    mask = torch.from_numpy(np.concatenate(mask, axis=0)).to(device=device)\n",
    "\n",
    "    return s, a, r, d, rtg, timesteps, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data_and_activation(\n",
    "    seed=666,\n",
    "    model_name='gpt2',\n",
    "    epoch=40,\n",
    "    env_name_list=['hopper', 'halfcheetah', 'walker2d'],\n",
    "    ):\n",
    "\n",
    "    for env_name in env_name_list:\n",
    "        \n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        dataset_name = 'medium'\n",
    "\n",
    "        if model_name == 'gpt2':\n",
    "            pretrained_lm1 = 'gpt2'\n",
    "        elif model_name == 'clip':\n",
    "            pretrained_lm1 = 'openai/clip-vit-base-patch32'\n",
    "        elif model_name == 'igpt':\n",
    "            pretrained_lm1 = 'openai/imagegpt-small'\n",
    "        elif model_name == 'dt':\n",
    "            pretrained_lm1 = False\n",
    "\n",
    "        variant = {\n",
    "            'embed_dim': 768,\n",
    "            'n_layer': 12,\n",
    "            'n_head': 1,\n",
    "            'activation_function': 'relu',\n",
    "            'dropout': 0.2, # 0.1\n",
    "            'load_checkpoint': False if epoch==0 else f'../checkpoints/{model_name}_medium_{env_name}_666/model_{epoch}.pt',\n",
    "            'seed': seed,\n",
    "            'outdir': f\"checkpoints/{model_name}_{dataset_name}_{env_name}_{seed}\",\n",
    "            'env': env_name,\n",
    "            'dataset': dataset_name,\n",
    "            'model_type': 'dt',\n",
    "            'K': 20, # 2\n",
    "            'pct_traj': 1.0,\n",
    "            'batch_size': 100,  # 64\n",
    "            'num_eval_episodes': 100,\n",
    "            'max_iters': 40,\n",
    "            'num_steps_per_iter': 2500,\n",
    "            'pretrained_lm': pretrained_lm1,\n",
    "            'gpt_kmeans': None,\n",
    "            'kmeans_cache': None,\n",
    "            'frozen': False,\n",
    "            'extend_positions': False,\n",
    "            'share_input_output_proj': True\n",
    "        }\n",
    "\n",
    "        os.makedirs(variant[\"outdir\"], exist_ok=True)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        trajectories, sorted_inds, state_dim, act_dim, max_ep_len, state_mean, state_std, num_trajectories, p_sample, scale = prepare_data(variant)\n",
    "\n",
    "        K = variant[\"K\"]\n",
    "        batch_size = variant[\"batch_size\"]\n",
    "\n",
    "        states, actions, rewards, dones, rtg, timesteps, attention_mask = get_batch(batch_size, \n",
    "                                                                                    K,\n",
    "                                                                                    trajectories,\n",
    "                                                                                    sorted_inds,\n",
    "                                                                                    state_dim,\n",
    "                                                                                    act_dim,\n",
    "                                                                                    max_ep_len,\n",
    "                                                                                    state_mean,\n",
    "                                                                                    state_std,\n",
    "                                                                                    num_trajectories,\n",
    "                                                                                    p_sample,\n",
    "                                                                                    scale,\n",
    "                                                                                    device\n",
    "                                                                                   )\n",
    "\n",
    "        data = {\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'rtg': rtg,\n",
    "            'timesteps': timesteps,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "\n",
    "        model = DecisionTransformer(\n",
    "            args=variant,\n",
    "            state_dim=state_dim,\n",
    "            act_dim=act_dim,\n",
    "            max_length=K,\n",
    "            max_ep_len=max_ep_len,\n",
    "            hidden_size=variant[\"embed_dim\"],\n",
    "            n_layer=variant[\"n_layer\"],\n",
    "            n_head=variant[\"n_head\"],\n",
    "            n_inner=4 * variant[\"embed_dim\"],\n",
    "            activation_function=variant[\"activation_function\"],\n",
    "            n_positions=1024,\n",
    "            resid_pdrop=variant[\"dropout\"],\n",
    "            attn_pdrop=0.1,\n",
    "        )\n",
    "        if variant[\"load_checkpoint\"]:\n",
    "            state_dict = torch.load(variant[\"load_checkpoint\"])  # , map_location=torch.device('cpu')\n",
    "            model.load_state_dict(state_dict)\n",
    "            print(f\"Loaded from {variant['load_checkpoint']}\")\n",
    "\n",
    "        model.to('cuda')\n",
    "        model.eval()\n",
    "\n",
    "        activation = {}\n",
    "        def get_activation(name):\n",
    "            def hook(model, input, output):\n",
    "                activation[name] = output.detach()\n",
    "            return hook\n",
    "\n",
    "        for block_id in range(len(model.transformer.h)):\n",
    "            model.transformer.h[block_id].ln_1.register_forward_hook(get_activation(f'{block_id}.ln_1'))\n",
    "            model.transformer.h[block_id].attn.c_attn.register_forward_hook(get_activation(f'{block_id}.attn.c_attn'))\n",
    "            model.transformer.h[block_id].attn.c_proj.register_forward_hook(get_activation(f'{block_id}.attn.c_proj'))\n",
    "            model.transformer.h[block_id].attn.attn_dropout.register_forward_hook(get_activation(f'{block_id}.attn.attn_dropout'))\n",
    "            model.transformer.h[block_id].attn.resid_dropout.register_forward_hook(get_activation(f'{block_id}.attn.resid_dropout'))\n",
    "            model.transformer.h[block_id].ln_2.register_forward_hook(get_activation(f'{block_id}.ln_2'))\n",
    "            model.transformer.h[block_id].mlp.c_fc.register_forward_hook(get_activation(f'{block_id}.mlp.c_fc'))\n",
    "            model.transformer.h[block_id].mlp.c_proj.register_forward_hook(get_activation(f'{block_id}.mlp.c_proj'))\n",
    "            # model.transformer.h[block_id].mlp.act.register_forward_hook(get_activation(f'{block_id}.mlp.act'))  # actはfunctionらしくregister_forward_hookがないと言われる\n",
    "            model.transformer.h[block_id].mlp.dropout.register_forward_hook(get_activation(f'{block_id}.mlp.dropout'))\n",
    "\n",
    "        state_preds, action_preds, reward_preds, all_embs = model.forward(\n",
    "            states,\n",
    "            actions,\n",
    "            rewards,\n",
    "            rtg[:, :-1],\n",
    "            timesteps,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "\n",
    "        activation_ordered = {}\n",
    "        block_name_list = [\n",
    "            'ln_1',\n",
    "            'attn.c_attn',\n",
    "            'attn.c_proj',\n",
    "            'attn.resid_dropout',\n",
    "            'ln_2',\n",
    "            'mlp.c_fc',\n",
    "            'mlp.c_proj',\n",
    "            'mlp.dropout'\n",
    "        ]\n",
    "        for block_id in range(len(model.transformer.h)):\n",
    "            for block_name in block_name_list:\n",
    "                activation_ordered[f'{block_id}.{block_name}'] = activation[f'{block_id}.{block_name}']\n",
    "        batch_size = variant['batch_size']\n",
    "        np.save(f'results/activation_{epoch}_{model_name}_{env_name}_{dataset_name}_{seed}_{batch_size}.npy', activation_ordered)\n",
    "        np.save(f'data/data_{env_name}_{dataset_name}_{seed}_{batch_size}.npy', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: halfcheetah medium\n",
      "1000 trajectories, 1000000 timesteps found\n",
      "Average return: 4770.33, std: 355.75\n",
      "Max return: 5309.38, min: -310.23\n",
      "==================================================\n",
      "Loading from pretrained\n",
      "Loaded from ../checkpoints/gpt2_medium_halfcheetah_666/model_40.pt\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 6.66 GiB already allocated; 41.38 MiB free; 6.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=0'>1</a>\u001b[0m save_data_and_activation(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=1'>2</a>\u001b[0m     seed\u001b[39m=\u001b[39;49m\u001b[39m666\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=2'>3</a>\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mgpt2\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m     epoch\u001b[39m=\u001b[39;49m\u001b[39m40\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m     env_name_list\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mhalfcheetah\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m     )\n",
      "\u001b[1;32m/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb Cell 4'\u001b[0m in \u001b[0;36msave_data_and_activation\u001b[0;34m(seed, model_name, epoch, env_name_list)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=117'>118</a>\u001b[0m     \u001b[39m# model.transformer.h[block_id].mlp.act.register_forward_hook(get_activation(f'{block_id}.mlp.act'))  # actはfunctionらしくregister_forward_hookがないと言われる\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=118'>119</a>\u001b[0m     model\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh[block_id]\u001b[39m.\u001b[39mmlp\u001b[39m.\u001b[39mdropout\u001b[39m.\u001b[39mregister_forward_hook(get_activation(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mblock_id\u001b[39m}\u001b[39;00m\u001b[39m.mlp.dropout\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=120'>121</a>\u001b[0m state_preds, action_preds, reward_preds, all_embs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mforward(\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=121'>122</a>\u001b[0m     states,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=122'>123</a>\u001b[0m     actions,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=123'>124</a>\u001b[0m     rewards,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=124'>125</a>\u001b[0m     rtg[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m],\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=125'>126</a>\u001b[0m     timesteps,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=126'>127</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=127'>128</a>\u001b[0m )\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=129'>130</a>\u001b[0m activation_ordered \u001b[39m=\u001b[39m {}\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=130'>131</a>\u001b[0m block_name_list \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=131'>132</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mln_1\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=132'>133</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mattn.c_attn\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=138'>139</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmlp.dropout\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeepcore/root/projects/can-wikipedia-help-offline-rl/code/notebooks/save_activation.ipynb#ch0000003vscode-remote?line=139'>140</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py:179\u001b[0m, in \u001b[0;36mDecisionTransformer.forward\u001b[0;34m(self, states, actions, rewards, returns_to_go, timesteps, attention_mask, past_key_values)\u001b[0m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=171'>172</a>\u001b[0m stacked_attention_mask \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=172'>173</a>\u001b[0m     torch\u001b[39m.\u001b[39mstack((attention_mask, attention_mask, attention_mask), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=173'>174</a>\u001b[0m     \u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=174'>175</a>\u001b[0m     \u001b[39m.\u001b[39mreshape(batch_size, \u001b[39m3\u001b[39m \u001b[39m*\u001b[39m seq_length)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=175'>176</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=177'>178</a>\u001b[0m \u001b[39m# we feed in the input embeddings (not word indices as in NLP) to the model\u001b[39;00m\n\u001b[0;32m--> <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=178'>179</a>\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=179'>180</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mstacked_inputs,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=180'>181</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mstacked_attention_mask,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=181'>182</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,  \u001b[39m# self.past_key_values,\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=182'>183</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=183'>184</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=184'>185</a>\u001b[0m x \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m\"\u001b[39m\u001b[39mlast_hidden_state\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/decision_transformer.py?line=185'>186</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpast_key_values \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m\"\u001b[39m\u001b[39mpast_key_values\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py:967\u001b[0m, in \u001b[0;36mGPT2Model.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=956'>957</a>\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=957'>958</a>\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=958'>959</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=963'>964</a>\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=964'>965</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=965'>966</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=966'>967</a>\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=967'>968</a>\u001b[0m         hidden_states,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=968'>969</a>\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=969'>970</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=970'>971</a>\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=971'>972</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=972'>973</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=973'>974</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=974'>975</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=975'>976</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=977'>978</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=978'>979</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py:476\u001b[0m, in \u001b[0;36mGPT2Block.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=473'>474</a>\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=474'>475</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_2(hidden_states)\n\u001b[0;32m--> <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=475'>476</a>\u001b[0m feed_forward_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmlp(hidden_states)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=476'>477</a>\u001b[0m \u001b[39m# residual connection\u001b[39;00m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=477'>478</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m feed_forward_hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1105'>1106</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1106'>1107</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1107'>1108</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1108'>1109</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1109'>1110</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1110'>1111</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   <a href='file:///usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py?line=1111'>1112</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py:400\u001b[0m, in \u001b[0;36mGPT2MLP.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=397'>398</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states):\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=398'>399</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_fc(hidden_states)\n\u001b[0;32m--> <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=399'>400</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mact(hidden_states)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=400'>401</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mc_proj(hidden_states)\n\u001b[1;32m    <a href='file:///root/projects/can-wikipedia-help-offline-rl/code/decision_transformer/models/trajectory_gpt2.py?line=401'>402</a>\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/activations.py:42\u001b[0m, in \u001b[0;36mgelu_new\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgelu_new\u001b[39m(x):\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=37'>38</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=38'>39</a>\u001b[0m \u001b[39m    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=39'>40</a>\u001b[0m \u001b[39m    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\u001b[39;00m\n\u001b[1;32m     <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=40'>41</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///usr/local/lib/python3.8/dist-packages/transformers/activations.py?line=41'>42</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m x \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39mtanh(math\u001b[39m.\u001b[39msqrt(\u001b[39m2.0\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39mpi) \u001b[39m*\u001b[39m (x \u001b[39m+\u001b[39m \u001b[39m0.044715\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mpow(x, \u001b[39m3.0\u001b[39;49m))))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 11.93 GiB total capacity; 6.66 GiB already allocated; 41.38 MiB free; 6.84 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "save_data_and_activation(\n",
    "    seed=666,\n",
    "    model_name='gpt2',\n",
    "    epoch=40,\n",
    "    env_name_list=['halfcheetah'],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
