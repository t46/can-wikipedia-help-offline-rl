{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb10e383",
   "metadata": {},
   "source": [
    "# Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28453ccf-521f-4f2d-9e4a-6d84ef48db7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Flow failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'flow'\n",
      "Warning: CARLA failed to import. Set the environment variable D4RL_SUPPRESS_IMPORT_ERROR=1 to suppress this message.\n",
      "No module named 'carla'\n",
      "pybullet build time: Mar  7 2022 18:18:14\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "from pprint import pprint\n",
    "\n",
    "import gym\n",
    "import d4rl\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a4395f-0b63-415a-bf82-7ef993196f1a",
   "metadata": {},
   "source": [
    "## Get Runs from Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "913a0b4e-f705-4323-b7f2-36ba5d996318",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'igpt'\n",
    "env_name = 'walker2d'\n",
    "seed = 42\n",
    "\n",
    "path = 'st46/decision-transformer'\n",
    "api = wandb.Api()\n",
    "runs = api.runs(\n",
    "    path=path,\n",
    "    filters={'state':'finished'}\n",
    "    )\n",
    "\n",
    "for run in runs:\n",
    "    if run.name == f'gym-experiment-{env_name}-medium-{model_name}-{seed}':\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0685c6ce-8a26-4145-9993-382d352599cc",
   "metadata": {},
   "source": [
    "## Get Return from Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "326afb45-e260-46d8-a1f9-1a7351e9f351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if env_name == 'hopper':\n",
    "    rtg_conditioning = 3600\n",
    "elif env_name == 'halfcheetah':\n",
    "    rtg_conditioning = 6000\n",
    "elif env_name == 'walker2d':\n",
    "    rtg_conditioning = 5000\n",
    "else:\n",
    "    rtg_conditioning = None\n",
    "\n",
    "return_map = {}\n",
    "return_map['medium'] = max(run.history()[f'evaluation/target_{rtg_conditioning}_return_mean'])\n",
    "best_checkpoint_epoch = np.argmax(run.history()[f'evaluation/target_{rtg_conditioning}_return_mean']) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6238b16-ef18-4ffa-95ba-241502df890b",
   "metadata": {},
   "source": [
    "## Get Return of Random and Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef3d01e0-fcb3-4cf7-a6d6-a82d7f62d6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Starting new experiment: walker2d random\n",
      "48907 trajectories, 999997 timesteps found\n",
      "Average return: 1.87, std: 5.81\n",
      "Max return: 75.03, min: -17.01\n",
      "==================================================\n",
      "==================================================\n",
      "Starting new experiment: walker2d expert\n",
      "1000 trajectories, 999214 timesteps found\n",
      "Average return: 4920.51, std: 136.39\n",
      "Max return: 5011.69, min: 763.42\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "datasets = [\"random\", \"expert\"]\n",
    "\n",
    "for dataset in datasets:\n",
    "\n",
    "    if env_name == \"hopper\":\n",
    "        env = gym.make(\"Hopper-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [3600, 1800]  # evaluation conditioning targets\n",
    "        scale = 1000.0  # normalization for rewards/returns\n",
    "    elif env_name == \"halfcheetah\":\n",
    "        env = gym.make(\"HalfCheetah-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [12000, 6000]\n",
    "        scale = 1000.0\n",
    "    elif env_name == \"walker2d\":\n",
    "        env = gym.make(\"Walker2d-v3\")\n",
    "        max_ep_len = 1000\n",
    "        env_targets = [5000, 2500]\n",
    "        scale = 1000.0\n",
    "    elif env_name == \"reacher2d\":\n",
    "        from decision_transformer.envs.reacher_2d import Reacher2dEnv\n",
    "\n",
    "        env = Reacher2dEnv()\n",
    "        max_ep_len = 100\n",
    "        env_targets = [76, 40]\n",
    "        scale = 10.0\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "\n",
    "    # load dataset\n",
    "    dataset_path = f\"../data/{env_name}-{dataset}-v2.pkl\"\n",
    "    with open(dataset_path, \"rb\") as f:\n",
    "        trajectories = pickle.load(f)\n",
    "\n",
    "    # save all path information into separate lists\n",
    "    mode = \"normal\"\n",
    "    states, traj_lens, returns = [], [], []\n",
    "    for path in trajectories:\n",
    "        if mode == \"delayed\":  # delayed: all rewards moved to end of trajectory\n",
    "            path[\"rewards\"][-1] = path[\"rewards\"].sum()\n",
    "            path[\"rewards\"][:-1] = 0.0\n",
    "        states.append(path[\"observations\"])\n",
    "        traj_lens.append(len(path[\"observations\"]))\n",
    "        returns.append(path[\"rewards\"].sum())\n",
    "    traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "\n",
    "    # used for input normalization\n",
    "    states = np.concatenate(states, axis=0)\n",
    "    state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "\n",
    "    num_timesteps = sum(traj_lens)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Starting new experiment: {env_name} {dataset}\")\n",
    "    print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "    print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "    print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if dataset == 'random':\n",
    "        return_map['random'] = np.mean(returns)\n",
    "    elif dataset == 'expert':\n",
    "        return_map['expert'] = np.mean(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f2b1b-5f95-403b-b3c3-9034a856186c",
   "metadata": {},
   "source": [
    "## Show Normalized Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4a42138-4a0f-4e5f-825d-f866d312a7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'igpt-walker2d-42'\n",
      "{'expert': 4920.507, 'medium': 305.231070789049, 'random': 1.871351}\n",
      "Epoch: 8\n",
      "Normalized Score: 6.167558766315959\n"
     ]
    }
   ],
   "source": [
    "pprint(f'{model_name}-{env_name}-{seed}')\n",
    "pprint(return_map)\n",
    "normalized_score = 100 * (return_map['medium'] - return_map['random']) / (return_map['expert'] - return_map['random'])\n",
    "print(f'Epoch: {best_checkpoint_epoch}')\n",
    "print(f'Normalized Score: {normalized_score}')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07f70ea1113ee523b3efee6307f7578bdf4830bd47c6db9199b4b48d8758cb04"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('wikirl-gym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
