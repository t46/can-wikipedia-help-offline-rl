{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Distance Analysis\n",
    "Compare attention distance between GPT2 and the randomly initialized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "sys.path.append('../')\n",
    "from sample_batch_data import get_data_info, get_batch\n",
    "from signal_propagation import get_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "sns.set_context(\"paper\", 1.5, {\"lines.linewidth\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Attention Distance Gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=666\n",
    "models=['gpt2', 'dt']\n",
    "epoch1=0\n",
    "epoch2=4\n",
    "env_name='hopper'\n",
    "path_to_model_checkpoint = 'path_to_model_checkpoint'\n",
    "path_to_save_att_dist_diff = 'path_to_save_att_dist_diff'\n",
    "path_to_save_figure = 'path_to_save_figure'\n",
    "path_to_dataset = 'path_to_dataset'\n",
    "\n",
    "dataset_name = 'medium'\n",
    "\n",
    "att_dist_diff_abs_list = []\n",
    "\n",
    "for model_name in tqdm(models):\n",
    "    model1 = model_name\n",
    "    model2 = model_name\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if model1 == 'gpt2':\n",
    "        pretrained_lm1 = 'gpt2'\n",
    "    elif model1 == 'clip':\n",
    "        pretrained_lm1 = 'openai/clip-vit-base-patch32'\n",
    "    elif model1 == 'igpt':\n",
    "        pretrained_lm1 = 'openai/imagegpt-small'\n",
    "    elif model1 == 'dt':\n",
    "        pretrained_lm1 = False\n",
    "\n",
    "    variant = {\n",
    "        'embed_dim': 768,\n",
    "        'n_layer': 12,\n",
    "        'n_head': 1,\n",
    "        'activation_function': 'relu',\n",
    "        'dropout': 0.2, # 0.1\n",
    "        'load_checkpoint': False if epoch1==0 else f'{path_to_model_checkpoint}/{model1}_medium_{env_name}_{seed}_K1/model_{epoch1}.pt',\n",
    "        'seed': seed,\n",
    "        'outdir': f\"tmp/{model1}_{dataset_name}_{env_name}_{seed}\",\n",
    "        'env': env_name,\n",
    "        'dataset': dataset_name,\n",
    "        'model_type': 'dt',\n",
    "        'K': 20, # 2\n",
    "        'pct_traj': 1.0,\n",
    "        'batch_size': 100,  # 64\n",
    "        'num_eval_episodes': 100,\n",
    "        'max_iters': 40,\n",
    "        'num_steps_per_iter': 2500,\n",
    "        'pretrained_lm': pretrained_lm1,\n",
    "        'gpt_kmeans': None,\n",
    "        'kmeans_cache': None,\n",
    "        'frozen': False,\n",
    "        'extend_positions': False,\n",
    "        'share_input_output_proj': True\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    state_dim, act_dim, max_ep_len, scale = get_data_info(variant)\n",
    "    states, actions, rewards, dones, rtg, timesteps, attention_mask = get_batch(variant, state_dim, act_dim, max_ep_len, scale, device, path_to_dataset)\n",
    "\n",
    "    att_dist_mat_list = []\n",
    "    for _ in tqdm(range(2)):\n",
    "\n",
    "        activation = get_activation(variant, state_dim, act_dim, max_ep_len, states, actions, rewards, rtg, timesteps, attention_mask, device)\n",
    "\n",
    "        if model2 == 'gpt2':\n",
    "            pretrained_lm2 = 'gpt2'\n",
    "        elif model2 == 'clip':\n",
    "            pretrained_lm2 = 'openai/clip-vit-base-patch32'\n",
    "        elif model2 == 'igpt':\n",
    "            pretrained_lm2 = 'openai/imagegpt-small'\n",
    "        elif model2 == 'dt':\n",
    "            pretrained_lm2 = False\n",
    "\n",
    "        variant['load_checkpoint'] = False if epoch2==0 else f'{path_to_model_checkpoint}/{model2}_medium_{env_name}_{seed}_K1/model_{epoch2}.pt'\n",
    "        variant['outdir'] =  f\"tmp/{model2}_{dataset_name}_{env_name}_{seed}\"\n",
    "        variant['pretrained_lm'] = pretrained_lm2\n",
    "\n",
    "        att_dist_mat = []\n",
    "        layers = np.arange(24) if model_name  == 'igpt' else np.arange(12)\n",
    "\n",
    "        for batchid in range(variant[\"batch_size\"]):\n",
    "\n",
    "            att_dist_list = []\n",
    "\n",
    "            for layer in layers:\n",
    "\n",
    "                query = activation[f'{layer}.attn.c_attn'][batchid, :, :768]  # 0, 5, 11\n",
    "                key = activation[f'{layer}.attn.c_attn'][batchid, :, 768: 1536]  # 0, 5, 11\n",
    "\n",
    "                att_weight_mat = torch.tril(torch.softmax(torch.matmul(query, key.t()) / np.sqrt(768), dim=1))\n",
    "\n",
    "                dist_mat = np.zeros_like(att_weight_mat)\n",
    "                for i in range(dist_mat.shape[0]):\n",
    "                    for j in range(dist_mat.shape[1]):\n",
    "                        dist_mat[i, j] = abs(i - j)\n",
    "\n",
    "                dist_mat *= np.array(att_weight_mat)\n",
    "                att_dist = np.mean(dist_mat)\n",
    "\n",
    "                att_dist_list.append(att_dist)\n",
    "\n",
    "            att_dist_mat.append(att_dist_list)\n",
    "\n",
    "        att_dist_mat_list.append(att_dist_mat)\n",
    "\n",
    "    att_dist_diff_abs = np.abs(np.array(att_dist_mat_list[0]) - np.array(att_dist_mat_list[1]))\n",
    "\n",
    "    np.save(f'{path_to_save_att_dist_diff}/att_dist_diff_{epoch1}_{epoch2}_{model_name}_{env_name}_{dataset_name}_{seed}_K1.npy', att_dist_diff_abs)\n",
    "\n",
    "    if model_name == 'igpt':\n",
    "        att_dist_diff_abs_normalized = np.zeros((variant[\"batch_size\"], len(layers) // 2))\n",
    "        for batch_id in range(variant[\"batch_size\"]):\n",
    "            att_dist_diff_abs_normalized_persample = []\n",
    "            for i in range(att_dist_diff_abs_normalized.shape[1]):\n",
    "                att_dist_diff_abs_normalized_persample.append(att_dist_diff_abs[batch_id, i * 2: (i + 1) * 2].mean())\n",
    "            att_dist_diff_abs_normalized[batch_id, :] = att_dist_diff_abs_normalized_persample\n",
    "        att_dist_diff_abs = att_dist_diff_abs_normalized\n",
    "\n",
    "    att_dist_diff_abs_list.append(att_dist_diff_abs)\n",
    "\n",
    "att_dist_diff_abs_cat = list(np.concatenate(att_dist_diff_abs_list).flatten())\n",
    "model_name = ['GPT2' for _ in range(1200)] + ['Random Init' for _ in range(1200)]\n",
    "block_id = list(np.array([[i for i in range(12)] for _ in range(200)]).flatten())\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'att_dist': att_dist_diff_abs_cat,\n",
    "    'model_name': model_name,\n",
    "    'block_id': block_id\n",
    "})\n",
    "\n",
    "sns.boxplot(x='block_id', y='att_dist', data=df, hue='model_name', palette={\"GPT2\": (0.372, 0.537, 0.537), \"Random Init\": (0.733, 0.737, 0.870)})\n",
    "plt.xlabel('Transformer Block')\n",
    "plt.ylabel(r'$|d_{att}$' + f'({epoch2}) - ' + r'$d_{att}$' + f'({epoch1})|')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{path_to_save_figure}/att_dist_diff_{epoch1}_{epoch2}_gpt2_dt_{env_name}_{dataset_name}_{seed}_K1.pdf')\n",
    "plt.show()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
